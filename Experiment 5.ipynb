{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982fae8-aaeb-4246-b72a-1f174ffb7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### make sure you install pyspark\n",
    "!pip install -U pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289abe84-f67b-4606-a415-474174738ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing libraeis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc15abe-ea4d-49e5-9ab1-2efda13686f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 10:10:13 WARN Utils: Your hostname, nooman resolves to a loopback address: 127.0.1.1; using 192.168.15.39 instead (on interface enp4s0)\n",
      "24/09/25 10:10:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/25 10:10:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# creating spark session\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Experiment 5\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .config(\"spark.executer\", \"3\")\n",
    "        .config(\"spark.executer.memory\", \"1g\")\n",
    "        .getOrCreate()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51043a5-aa57-4b58-8824-7f53e5295a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('CustomerID', IntegerType(), True), StructField('FirstName', StringType(), True), StructField('LastName', StringType(), True), StructField('Date_of_Birth', TimestampType(), True), StructField('City', StringType(), True), StructField('State', StringType(), True), StructField('Country', StringType(), True), StructField('PostalCode', IntegerType(), True), StructField('Phone', LongType(), True), StructField('Email', StringType(), True), StructField('DateEntered', TimestampType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(\"/Ecommerce/Customers/Customers.csv\", inferSchema=True, header=True).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4783c66-acab-4ae9-b041-ac9c27fd9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StringType, DateType, StructType\n",
    "\n",
    "customers_schema = StructType(\n",
    "    [\n",
    "        StructField('CustomerID', IntegerType(), True),\n",
    "        StructField('FirstName', StringType(), True),\n",
    "        StructField('LastName', StringType(), True),\n",
    "        StructField('Date_of_Birth', DateType(), True),\n",
    "        StructField('City', StringType(), True),\n",
    "        StructField('State', StringType(), True),\n",
    "        StructField('Country', StringType(), True),\n",
    "        StructField('PostalCode', StringType(), True),\n",
    "        StructField('Phone', StringType(), True),\n",
    "        StructField('Email', StringType(), True),\n",
    "        StructField('DateEntered', DateType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "orders_schema = \"\"\"\n",
    "OrderID INT,\n",
    "CustomerID INT,\n",
    "PaymentID INT,\n",
    "OrderDate DATE,\n",
    "ShipperID INT,\n",
    "ShipDate Date,\n",
    "DeliveryDate Date, \n",
    "Total_order_amount float\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "770a5273-9734-48ce-b8b5-81e2a8b7f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am reading loading from hdfs if you don't have hadoop in your system\n",
    "# download the datasets usign this link \n",
    "## https://drive.google.com/drive/folders/1VzW2eBDCjOc4zzG-dSObFLyQxBSWVbn2\n",
    "customers = spark.read.csv(\"/Ecommerce/Customers/Customers.csv\", schema=customers_schema, header=True)\n",
    "orders = spark.read.csv(\"/Ecommerce/Orders/Orders.csv\", schema=orders_schema, header=True)\n",
    "payments = spark.read.csv(\"/Ecommerce/Payments/Payments.csv\", inferSchema=True, header=True)\n",
    "products = spark.read.csv(\"/Ecommerce/Products/Products.csv\", inferSchema=True, header=True)\n",
    "orderdetails = spark.read.csv(\"/Ecommerce/OrderDetails/OrderDetails.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b7f631-218d-4e0a-b515-bd9652788b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a temporary table which is volatile when saprk session is close it will gone if you will \n",
    "# create global temporary it will store into Hive metastore\n",
    "\n",
    "customers.createOrReplaceTempView(\"Customers\")\n",
    "orders.createOrReplaceTempView(\"Orders\")\n",
    "payments.createOrReplaceTempView(\"Payments\")\n",
    "products.createOrReplaceTempView(\"Products\")\n",
    "orderdetails.createOrReplaceTempView(\"OrderDetails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116f7741-3d58-4b83-871b-3833daebf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------+--------+--------+-------------+----------+----------+--------------------------+-----------+\n",
      "|CustomerID|FirstName|LastName |Date_of_Birth|City    |State   |Country      |PostalCode|Phone     |Email                     |DateEntered|\n",
      "+----------+---------+---------+-------------+--------+--------+-------------+----------+----------+--------------------------+-----------+\n",
      "|57081     |James    |Smith    |1987-03-26   |New York|New York|United States|280862    |9638483934|James.Smith@gmail.com     |2020-01-02 |\n",
      "|57082     |Robert   |Downey Jr|1973-05-24   |New York|New York|United States|376573    |6588282115|Robert.Downey Jr@gmail.com|2020-01-06 |\n",
      "+----------+---------+---------+-------------+--------+--------+-------------+----------+----------+--------------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ad5e6c-d404-4e83-9281-49b1053da95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+---------+----------+------------+------------------+\n",
      "|OrderID|CustomerID|PaymentID|OrderDate |ShipperID|ShipDate  |DeliveryDate|Total_order_amount|\n",
      "+-------+----------+---------+----------+---------+----------+------------+------------------+\n",
      "|7655500|57083     |2        |2020-01-12|7        |2020-01-13|2020-01-19  |25112.0           |\n",
      "|7655501|57086     |3        |2020-01-20|2        |2020-01-24|2020-01-27  |22453.0           |\n",
      "+-------+----------+---------+----------+---------+----------+------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c27e46-981f-4a9b-aa61-98ce9239e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------------------+-----------+--------------------+------+----------+------------+-----------------------------+\n",
      "|ProductID|Product                                    |Category_ID|Sub_Category        |Brand |Sale_Price|Market_Price|Type                         |\n",
      "+---------+-------------------------------------------+-----------+--------------------+------+----------+------------+-----------------------------+\n",
      "|1        |Original Disinfectant Toilet Cleaner Liquid|5001       |All Purpose Cleaners|Harpic|489       |534         |Toilet Cleaners              |\n",
      "|2        |Surface Disinfectant Spray                 |5001       |All Purpose Cleaners|Savlon|257       |318         |Disinfectant Spray & Cleaners|\n",
      "+---------+-------------------------------------------+-----------+--------------------+------+----------+------------+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a796cb98-6b16-4fcb-9bf9-9cb0b20624cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+\n",
      "|PaymentID|PaymentType|Allowed|\n",
      "+---------+-----------+-------+\n",
      "|        1| Debit Card|    Yes|\n",
      "|        2|        POD|    Yes|\n",
      "+---------+-----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payments.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7916501-684b-481d-ad41-2579d8882e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+--------------------+------+----------+------------+--------------------+\n",
      "|ProductID|             Product|Category_ID|        Sub_Category| Brand|Sale_Price|Market_Price|                Type|\n",
      "+---------+--------------------+-----------+--------------------+------+----------+------------+--------------------+\n",
      "|        1|Original Disinfec...|       5001|All Purpose Cleaners|Harpic|       489|         534|     Toilet Cleaners|\n",
      "|        2|Surface Disinfect...|       5001|All Purpose Cleaners|Savlon|       257|         318|Disinfectant Spra...|\n",
      "+---------+--------------------+-----------+--------------------+------+----------+------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449f8b6c-dbe8-47cd-96e9-89cef407d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+---------+--------+----------+\n",
      "|OrderDetailID|OrderID|ProductID|Quantity|SupplierID|\n",
      "+-------------+-------+---------+--------+----------+\n",
      "|            1|7655500|    14955|       2|         3|\n",
      "|            2|7655500|    19946|       4|         2|\n",
      "+-------------+-------+---------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderdetails.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e51d3-e926-4b43-a840-4eec99ef01c0",
   "metadata": {},
   "source": [
    "##### Find the Monthly revenue and %change in revenue with previous month\n",
    "##### Find the product which is given 75% revneue of total\n",
    "##### Find the year monthly  wise revenue and commulative revenue\n",
    "##### Find the avg differnce between two consicative orders and divide into 3 bucket\n",
    "##### Most selling products in terms of quantity in each country\n",
    "##### check is there any trend every month some different product on peak\n",
    "##### avg deleivery time for each country rank them in descending orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf016f85-893d-4e5d-b4bf-1b92287b3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----------------------+----------+\n",
      "|Years|Months|Revenue|Previous_month_revenue|per_change|\n",
      "+-----+------+-------+----------------------+----------+\n",
      "| 2020|     1|  47565|                  NULL|      NULL|\n",
      "| 2020|     2| 200109|                 47565|     76.23|\n",
      "| 2020|     3| 318971|                200109|     37.26|\n",
      "| 2020|     4| 561036|                318971|     43.15|\n",
      "| 2020|     5| 891360|                561036|     37.06|\n",
      "| 2020|     6|1196040|                891360|     25.47|\n",
      "| 2020|     7|1061605|               1196040|    -12.66|\n",
      "| 2020|     8|1218623|               1061605|     12.88|\n",
      "| 2020|     9|1678720|               1218623|     27.41|\n",
      "| 2020|    10|1792856|               1678720|      6.37|\n",
      "| 2020|    11|2051862|               1792856|     12.62|\n",
      "| 2020|    12|2208324|               2051862|      7.09|\n",
      "| 2021|     1|2696530|                  NULL|      NULL|\n",
      "| 2021|     2|2611481|               2696530|     -3.26|\n",
      "| 2021|     3|4211055|               2611481|     37.99|\n",
      "| 2021|     4|4404256|               4211055|      4.39|\n",
      "| 2021|     5|5081762|               4404256|     13.33|\n",
      "| 2021|     6|5215665|               5081762|      2.57|\n",
      "| 2021|     7|7014047|               5215665|     25.64|\n",
      "| 2021|     8|6911452|               7014047|     -1.48|\n",
      "+-----+------+-------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the Monthly revenue and %change in revenue with previous month\n",
    "\n",
    "query = \"\"\"\n",
    "WITH CTE AS\n",
    "(\n",
    "    SELECT \n",
    "        YEAR(OrderDate) as Years,\n",
    "        MONTH(OrderDate) as Months,\n",
    "        CAST(SUM(Total_order_amount) AS INT) as Revenue\n",
    "    FROM Orders\n",
    "    GROUP BY YEAR(OrderDate), MONTH(OrderDate)\n",
    ")\n",
    "SELECT *, \n",
    "    LAG(Revenue, 1) OVER(PARTITION BY Years ORDER BY Months) AS Previous_month_revenue,\n",
    "    ROUND(((Revenue - LAG(Revenue, 1) OVER(PARTITION BY Years ORDER BY Months))/Revenue)*100, 2) AS per_change\n",
    "FROM CTE\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091db8d0-e80d-4d8b-95c9-c3ac44b551ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-----------------+---------------+\n",
      "|Years|Months|Revenue|previos_moth_sale|percent_channge|\n",
      "+-----+------+-------+-----------------+---------------+\n",
      "| 2020|     1|  47565|             NULL|           NULL|\n",
      "| 2020|     2| 200109|            47565|          76.23|\n",
      "| 2020|     3| 318971|           200109|          37.26|\n",
      "| 2020|     4| 561036|           318971|          43.15|\n",
      "| 2020|     5| 891360|           561036|          37.06|\n",
      "| 2020|     6|1196040|           891360|          25.47|\n",
      "| 2020|     7|1061605|          1196040|         -12.66|\n",
      "| 2020|     8|1218623|          1061605|          12.88|\n",
      "| 2020|     9|1678720|          1218623|          27.41|\n",
      "| 2020|    10|1792856|          1678720|           6.37|\n",
      "| 2020|    11|2051862|          1792856|          12.62|\n",
      "| 2020|    12|2208324|          2051862|           7.09|\n",
      "| 2021|     1|2696530|             NULL|           NULL|\n",
      "| 2021|     2|2611481|          2696530|          -3.26|\n",
      "| 2021|     3|4211055|          2611481|          37.99|\n",
      "| 2021|     4|4404256|          4211055|           4.39|\n",
      "| 2021|     5|5081762|          4404256|          13.33|\n",
      "| 2021|     6|5215665|          5081762|           2.57|\n",
      "| 2021|     7|7014047|          5215665|          25.64|\n",
      "| 2021|     8|6911452|          7014047|          -1.48|\n",
      "+-----+------+-------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "yoy_sales = (\n",
    "orders.groupBy(\n",
    "    F.year(F.col('orderdate')).alias(\"Years\"),\n",
    "    F.month(F.col('OrderDate')).alias(\"Months\")\n",
    ")\n",
    ".agg(F.sum(col(\"Total_order_amount\")).cast(\"int\").alias(\"Revenue\"))\n",
    ")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Years\").orderBy(\"Months\")\n",
    "\n",
    "yoy_sales.withColumns(\n",
    "    {\"previos_moth_sale\" : F.lag(col(\"Revenue\"), 1).over(windowSpec),\n",
    "    \"percent_channge\": F.round((col(\"Revenue\") - F.lag(col(\"Revenue\"), 1).over(windowSpec))/col(\"Revenue\") * 100, 2)}\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3645a0f6-abc4-40b0-895c-075d362e3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-------------------+\n",
      "|Years|Months|Revenue|Commulative_Revenue|\n",
      "+-----+------+-------+-------------------+\n",
      "| 2020|     1|  47565|              47565|\n",
      "| 2020|     2| 200109|             247674|\n",
      "| 2020|     3| 318971|             566645|\n",
      "| 2020|     4| 561036|            1127681|\n",
      "| 2020|     5| 891360|            2019041|\n",
      "| 2020|     6|1196040|            3215081|\n",
      "| 2020|     7|1061605|            4276686|\n",
      "| 2020|     8|1218623|            5495309|\n",
      "| 2020|     9|1678720|            7174029|\n",
      "| 2020|    10|1792856|            8966885|\n",
      "| 2020|    11|2051862|           11018747|\n",
      "| 2020|    12|2208324|           13227071|\n",
      "| 2021|     1|2696530|            2696530|\n",
      "| 2021|     2|2611481|            5308011|\n",
      "| 2021|     3|4211055|            9519066|\n",
      "| 2021|     4|4404256|           13923322|\n",
      "| 2021|     5|5081762|           19005084|\n",
      "| 2021|     6|5215665|           24220749|\n",
      "| 2021|     7|7014047|           31234796|\n",
      "| 2021|     8|6911452|           38146248|\n",
      "+-----+------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find the year monthly wise revenue and commulative revenue\n",
    "import seaborn as sns\n",
    "query = \"\"\"\n",
    "\n",
    "WITH CTE AS\n",
    "(\n",
    "SELECT \n",
    "    YEAR(OrderDate) AS Years,\n",
    "    MONTH(OrderDate) AS Months,\n",
    "    CAST(SUM(Total_order_amount) AS INT) AS Revenue\n",
    "FROM Orders\n",
    "GROUP BY YEAR(OrderDate), MONTH(OrderDate)\n",
    ")\n",
    "SELECT *,\n",
    "    SUM(Revenue) OVER(PARTITION BY Years ORDER BY Months) AS Commulative_Revenue\n",
    "FROM CTE\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82048896-5c55-49cf-b930-d6e23515636a",
   "metadata": {},
   "source": [
    "Find the top product which is given 10% revneue of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95085c8-bcbd-4cfd-ac42-76218a583523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/20 09:53:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|ProductID|             Product|           Revenue|\n",
      "+---------+--------------------+------------------+\n",
      "|    12576|Raw Manuka Honey ...|         309849.75|\n",
      "|    18730|Futura Hard Anodi...|          282753.0|\n",
      "|     4711|AQVA Divina Body ...|         275639.75|\n",
      "|     4638|Be Delicious Eau ...|    271510.8515625|\n",
      "|    19320|Lily Ville Square...|          257149.0|\n",
      "|    20295|Quantum Max Dishw...|     247793.953125|\n",
      "|     4139|New Imperial 1000...|          236096.0|\n",
      "|    12290|Chunky Orange Mar...|   234557.80078125|\n",
      "|     6927|Hot Water Rubber ...|234380.35009765625|\n",
      "|     4817|Voyage Sport Eau ...|     233160.078125|\n",
      "|    14645|       Bhringraj Oil| 231098.7998046875|\n",
      "|    15164|Healthy Khakhra -...|          229811.0|\n",
      "|    18568|Stainless Steel C...|          223229.0|\n",
      "|    20326|   Laundry Detergent|    220203.4765625|\n",
      "|    10939|Liver Tonic & App...|   220038.69921875|\n",
      "|      833|Baby Bottle & Foo...|          216971.0|\n",
      "|    10759|Adult Cat Food Po...|  213932.599609375|\n",
      "|     8586|Masala - Crispy C...|    213912.1015625|\n",
      "|    10784|Drool Dog Food Ad...|     209906.796875|\n",
      "|    13442|Tea Tree Essentia...|          209302.0|\n",
      "+---------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH CTE1 AS\n",
    "(\n",
    "SELECT \n",
    "    c.ProductID, \n",
    "    c.Product,\n",
    "    SUM(a.Total_order_amount) AS Revenue,\n",
    "    SUM(a.Total_order_amount)/(SELECT SUM(Total_order_amount) FROM Orders) * 100 AS percent_revenue\n",
    "FROM Orders AS a\n",
    "JOIN OrderDetails AS b\n",
    "ON a.OrderID = b.OrderID\n",
    "JOIN Products AS c\n",
    "ON b.ProductID = c.ProductID\n",
    "GROUP BY c.ProductID, c.Product\n",
    "),\n",
    "CTE2 AS\n",
    "(\n",
    "SELECT *,\n",
    "    SUM(percent_revenue) OVER(ORDER BY Revenue DESC) AS commulative_percent\n",
    "FROM CTE1\n",
    ")\n",
    "SELECT \n",
    "    ProductID,\n",
    "    Product,\n",
    "    Revenue\n",
    "FROM CTE2\n",
    "WHERE commulative_percent <= 10\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074bfb0-3d00-4ce5-9edb-028cfaaf3443",
   "metadata": {},
   "source": [
    "Find the avg differnce between two consicative orders and divide into 3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7067e680-fe02-43ab-95e4-e301b4efa75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------+\n",
      "|CustomerID|FirstName|LastName|avg_difference|\n",
      "+----------+---------+--------+--------------+\n",
      "|     57605|  Malcolm|  Julian|           0.0|\n",
      "|     57604|    Jesse| Francis|           1.0|\n",
      "|     57603|    Idris|   Colby|          1.14|\n",
      "|     57594|    River|   Louie|           1.5|\n",
      "|     57597|   Szymon|   Asher|          1.62|\n",
      "|     57601|  Darragh|   Niall|           1.7|\n",
      "|     57600|    Bruce|  Lennox|          1.73|\n",
      "|     57598|      Tom|  Bailey|           1.8|\n",
      "|     57602|     Dean|Alistair|          1.83|\n",
      "|     57596|  Stanley|     Ali|           2.0|\n",
      "|     57593|  Oliwier|   Dylan|          2.25|\n",
      "|     57592|    Munro|   Blair|           2.5|\n",
      "|     57599|    Tymon|    Enzo|           3.0|\n",
      "|     57595|    Ruari| Vincent|          3.57|\n",
      "|     57589|    Marco|   Arran|          3.75|\n",
      "|     57586|      Kit|  Hamish|           4.0|\n",
      "|     57583|   Conall|   David|           4.1|\n",
      "|     57585|     Ivor|    Ryan|          4.22|\n",
      "|     57574|  Richard|   Corey|          4.54|\n",
      "|     57584|    Haris|   Isaac|          4.57|\n",
      "+----------+---------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "customer_order = (\n",
    "customers.join(\n",
    "          orders,\n",
    "          on=\"CustomerID\")\n",
    "        .select(\"CustomerID\", \"FirstName\", \"LastName\", \"OrderID\", \"OrderDate\")\n",
    ")\n",
    "\n",
    "windowspec = Window.partitionBy(F.col(\"CustomerID\")).orderBy(F.col(\"OrderDate\"))\n",
    "difference = customer_order.withColumns(\n",
    "    {\n",
    "     \"previous_order\": F.lag(F.col(\"OrderDate\")).over(windowspec),\n",
    "     \"difference\": F.date_diff(F.col(\"OrderDate\"), F.lag(F.col(\"OrderDate\")).over(windowspec))\n",
    "    }\n",
    ")\n",
    "difference.groupBy(\n",
    "    F.col(\"CustomerID\"), \n",
    "    F.col(\"FirstName\"), \n",
    "    F.col(\"LastName\")).agg(\n",
    "    F.round(F.avg(F.col(\"difference\")), 2).alias(\"avg_difference\")\n",
    ").orderBy(F.col(\"avg_difference\")).filter(F.col(\"avg_difference\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f338986-b3be-4131-900e-ff7826b14137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------+\n",
      "|CustomerID|FirstName|LastName|avg_difference|\n",
      "+----------+---------+--------+--------------+\n",
      "|     57605|  Malcolm|  Julian|           0.0|\n",
      "|     57604|    Jesse| Francis|           1.0|\n",
      "|     57603|    Idris|   Colby|          1.14|\n",
      "|     57594|    River|   Louie|           1.5|\n",
      "|     57597|   Szymon|   Asher|          1.62|\n",
      "|     57601|  Darragh|   Niall|           1.7|\n",
      "|     57600|    Bruce|  Lennox|          1.73|\n",
      "|     57598|      Tom|  Bailey|           1.8|\n",
      "|     57602|     Dean|Alistair|          1.83|\n",
      "|     57596|  Stanley|     Ali|           2.0|\n",
      "|     57593|  Oliwier|   Dylan|          2.25|\n",
      "|     57592|    Munro|   Blair|           2.5|\n",
      "|     57599|    Tymon|    Enzo|           3.0|\n",
      "|     57595|    Ruari| Vincent|          3.57|\n",
      "|     57589|    Marco|   Arran|          3.75|\n",
      "|     57586|      Kit|  Hamish|           4.0|\n",
      "|     57583|   Conall|   David|           4.1|\n",
      "|     57585|     Ivor|    Ryan|          4.22|\n",
      "|     57574|  Richard|   Corey|          4.54|\n",
      "|     57584|    Haris|   Isaac|          4.57|\n",
      "+----------+---------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH CTE AS\n",
    "(\n",
    "SELECT \n",
    "    a.CustomerID,\n",
    "    FirstName,\n",
    "    LastName,\n",
    "    OrderID,\n",
    "    OrderDate,\n",
    "    LAG(OrderDate, 1) OVER(PARTITION BY a.CustomerID ORDER BY OrderDate) AS previous_orderdate,\n",
    "    DATEDIFF(OrderDate, LAG(OrderDate, 1) OVER(PARTITION BY a.CustomerID ORDER BY OrderDate)) AS difference\n",
    "FROM Customers AS a\n",
    "JOIN Orders AS b\n",
    "ON a.CustomerID = b.CustomerID\n",
    ")\n",
    "SELECT \n",
    "    CustomerID,\n",
    "    FirstName,\n",
    "    LastName,\n",
    "    ROUND(AVG(difference), 2) as avg_difference\n",
    "FROM CTE\n",
    "GROUP BY CustomerID, FirstName, LastName\n",
    "HAVING ROUND(AVG(difference), 2) IS NOT NULL\n",
    "ORDER BY avg_difference\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a05f38-9c45-4e1c-8f5f-743fd4e07347",
   "metadata": {},
   "source": [
    "##### Most selling products in terms of quantity in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "170b9f6c-8426-49f6-8391-9d6087bb4c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+--------------------+--------------+----+\n",
      "|         Country|ProductID|             Product|total_quantity|rank|\n",
      "+----------------+---------+--------------------+--------------+----+\n",
      "|       Australia|    21319|BBPopular Almond/...|            35|   1|\n",
      "|         Austria|    13902|    Skin Therapy Oil|            33|   1|\n",
      "|         Austria|    11912|           Pav Bhaji|            33|   1|\n",
      "|         Belgium|    14069|Honey & Rose Body...|            39|   1|\n",
      "|          France|     2184|Milk Shakti Biscu...|            42|   1|\n",
      "|         Germany|    12058|Rice - Ponni, Boiled|            37|   1|\n",
      "|          Greece|    15203|Kochi Masala Bana...|            53|   1|\n",
      "|           India|     3947|Liquid Food Colou...|            57|   1|\n",
      "|         Ireland|    17531|Wooden Printed Ir...|            39|   1|\n",
      "|           Italy|     1237|Glycerin Germ Pro...|            35|   1|\n",
      "|     Netherlands|      510|Alkaline Battery ...|            36|   1|\n",
      "|     New Zealand|    18142|Pickle - Crunchy,...|            30|   1|\n",
      "|Northern Ireland|    12304|   Sauce - Light Soy|            38|   1|\n",
      "|          Poland|    14203|Pure Radiance Fac...|            36|   1|\n",
      "|        Portugal|    19591|Cheese - Pepper J...|            51|   1|\n",
      "|         Romania|    19985|         Kool - Rose|            35|   1|\n",
      "|          Russia|    15215|      Roasted Peanut|            38|   1|\n",
      "|    South Africa|    10419|Organic - Quinoa ...|            20|   1|\n",
      "|    South Africa|    19109|Stainless Steel S...|            20|   1|\n",
      "|    South Africa|     5560| Thandai Kesar Badam|            20|   1|\n",
      "+----------------+---------+--------------------+--------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowspec = Window.partitionBy(F.col(\"Country\")).orderBy(F.col(\"total_quantity\").desc())\n",
    "(\n",
    "customers.join(orders, \n",
    "               on=\"CustomerID\")\n",
    "         .join(orderdetails, on=\"OrderID\")\n",
    "         .join(products, on=\"ProductID\")\n",
    "         .select(\"Country\", \"ProductID\", \"Product\",\"Quantity\")\n",
    "         .groupBy(\"Country\", \"ProductID\", \"Product\").agg(\n",
    "             F.sum(F.col(\"Quantity\")).alias(\"total_quantity\")\n",
    "         ).withColumn(\"rank\", F.dense_rank().over(windowspec))\n",
    "         .filter(F.col(\"rank\") == 1)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2b46d-b9a9-4d05-bde1-69c2d97364c7",
   "metadata": {},
   "source": [
    "#### check is there any trend every month some different product on peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed062644-9951-4e51-8ca8-0e3b7596df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+-------------+----+\n",
      "|months|ProductID|             Product|quantity_sold|rank|\n",
      "+------+---------+--------------------+-------------+----+\n",
      "|     1|    10481|Party Eye Mask - ...|           30|   1|\n",
      "|     2|      638|Orthodontic Sooth...|           34|   1|\n",
      "|     3|     6195|Taft Classic Hair...|           34|   1|\n",
      "|     3|    16921|Copper Steel Wate...|           34|   1|\n",
      "|     4|    17623|Square Pet water ...|           37|   1|\n",
      "|     5|    20009|Moong Dal/Hesaru ...|           39|   1|\n",
      "|     6|     6962|     Hot Water - Bag|           38|   1|\n",
      "|     7|    19599|        Pepper Gouda|           38|   1|\n",
      "|     8|     7965|Kashmiri Chilly P...|           40|   1|\n",
      "|     9|     3558|Dark Chocolate - ...|           45|   1|\n",
      "|    10|    20538|Ultra Soft Premiu...|           48|   1|\n",
      "|    11|    16108|Peanut Butter - C...|           49|   1|\n",
      "|    11|     9422|Cleaning Brush - ...|           49|   1|\n",
      "|    12|     6226|NHP 8103 Silky Sh...|           51|   1|\n",
      "+------+---------+--------------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(F.col(\"months\")).orderBy(F.col(\"quantity_sold\").desc())\n",
    "(\n",
    "    orders.join(orderdetails, on=\"OrderID\")\n",
    "    .join(products, on=\"ProductID\")\n",
    "    .groupby(F.month(F.col(\"OrderDate\")).alias(\"months\"), \n",
    "             F.col(\"ProductID\"),\n",
    "             F.col(\"Product\"))\n",
    "    .agg(\n",
    "        F.sum(F.col(\"Quantity\")).alias(\"quantity_sold\")\n",
    "    )\n",
    "    .withColumn(\"rank\", F.dense_rank().over(window_spec))\n",
    "    .filter(F.col(\"rank\") == 1)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb45568-892c-40a7-ad78-62bb1e7c1549",
   "metadata": {},
   "source": [
    "##### avg deleivery time for each country rank them in descending orders¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "823ede20-3baf-46e2-97eb-8b8e9f9d1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------------+----+\n",
      "|  Country|          City|     avg_lead_time|rank|\n",
      "+---------+--------------+------------------+----+\n",
      "|Australia|Sunshine Coast|10.833333333333334|   1|\n",
      "|Australia|     Newcastle|              11.5|   2|\n",
      "|Australia|      Canberra|12.333333333333334|   3|\n",
      "|Australia|       Geelong|13.142857142857142|   4|\n",
      "|Australia|        Cairns|14.210526315789474|   5|\n",
      "|Australia|      Brisbane|15.419354838709678|   6|\n",
      "|Australia| Central Coast|              15.5|   7|\n",
      "|Australia|      Ballarat| 15.76923076923077|   8|\n",
      "|Australia|     Melbourne|15.833333333333334|   9|\n",
      "|Australia|         Perth|              15.9|  10|\n",
      "|Australia|    Townsville| 16.42105263157895|  11|\n",
      "|Australia|        Darwin|16.454545454545453|  12|\n",
      "|Australia|        Hobart|17.863636363636363|  13|\n",
      "|Australia|      Adelaide| 18.41176470588235|  14|\n",
      "|Australia|    Wollongong|              18.6|  15|\n",
      "|Australia|        Sydney|             22.25|  16|\n",
      "|  Austria|     Innsbruck|12.555555555555555|   1|\n",
      "|  Austria|      Salzburg|14.631578947368421|   2|\n",
      "|  Austria|       Villach| 15.46808510638298|   3|\n",
      "|  Austria|          Graz|              16.3|   4|\n",
      "+---------+--------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowspec = Window.partitionBy(\"Country\").orderBy(F.col(\"avg_lead_time\"))\n",
    "(\n",
    " customers.join(orders, on=\"CustomerID\")\n",
    " .groupBy(\"Country\", \"City\").agg(\n",
    "     F.avg(F.date_diff(F.col(\"DeliveryDate\"), F.col(\"OrderDate\"))).alias(\"avg_lead_time\"))\n",
    "     .withColumn(\"rank\", F.row_number().over(windowspec))\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
